# Introduction

## About This Workshop

Retrieval-Augmented Generation (RAG) has traditionally been centered around text-based retrieval and generation. However, the integration of images into the RAG pipeline unlocks a new dimension of possibilities. By enabling systems to retrieve and incorporate visual data alongside textual information, image-based RAG creates richer, more dynamic, and contextually aware outputs tailored to real-world needs.

This Image-Focused Multimodal RAG LiveLab will guide you through the process of designing, implementing, and optimizing RAG systems that combine the strengths of visual and textual data. Whether itâ€™s enhancing a chatbot with relevant images, building a retrieval system for e-commerce product catalogs, or powering an AI assistant that combines text and visual answers, this workshop will provide you with the tools to master image-based RAG applications.

Estimated time: 240 minutes

Objectives

In this workshop, you will learn how to:

* Provision necessary infrastructure and platform services.
* Deploy a multimodal embedding model on OCI Data Science.
* Store images and their embeddings inside Oracle Autonomous 23ai.
* Extend Llama 3.2-90B capabilities using multimodal RAG.
* Host the complete multimodal RAG application on OCI Compute.

Prerequisites:
* Oracle Cloud account with access to the OCI Generative AI Service
* Privileges to create Autonomous Database, OCI Compute, OCI Data Science, Oracle Container Registry image.
* Limits for a single Data Science A10 GPU for performance reasons is helpful, but not required.
* Basic knowledge of Python is helpful.
* Basic knowledge of Linux is helpful.

Code used in this livelab can be found below:
* [GitHub Repository]()

## Learn More
* [OCI Generative AI Service](https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/)
* [Oracle AI Vector Search](https://www.oracle.com/database/ai-vector-search/)
* [OCI Data Science](https://www.oracle.com/artificial-intelligence/data-science/)

## Acknowledgements

* Author - Enis Aras
* Last Updated By/Date: Enis Aras, December 2024


